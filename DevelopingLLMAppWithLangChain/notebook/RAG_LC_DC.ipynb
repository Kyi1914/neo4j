{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the langchain_community package\n",
    "# %pip install langchain_community\n",
    "# %pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PDF document loader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"paht/to/file/attention_is_all_you_need.pdf\") # Load the PDF file\n",
    "data = loader.load() # Load the PDF file into memory\n",
    "print(data[0])\n",
    "print(loader.text) # Print the text of the PDF file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CSV document loader\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(\"fifa_countries_audience.csv\") # Load the PDF file\n",
    "\n",
    "data = loader.load() # Load the PDF file into memory\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HTML document loader\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "# loader = UnstructuredHTMLLoader(\"https://en.wikipedia.org/wiki/Deep_learning\") # Load the PDF file\n",
    "loader = UnstructuredHTMLLoader(\"white_house_executive_order_nov_2023\") # Load the PDF file\n",
    "\n",
    "data = loader.load() # Load the PDF file into memory\n",
    "print(data[0]) # Print the first document\n",
    "\n",
    "# Print the first document's metadata\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One machine can do the work of fifty ordinary humans. \n",
      "But no machine can do the work of one extraordinary human.\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "# quote\n",
    "quote = '''One machine can do the work of fifty ordinary humans. \\nBut no machine can do the work of one extraordinary human.'''\n",
    "print(quote)\n",
    "\n",
    "print(len(quote))\n",
    "\n",
    "chunk_size = 24\n",
    "chunk_overlap = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 52, which is longer than the specified 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the work of fifty ordinary humans', 'But no machine can do the work of one extraordinary human']\n",
      "[52, 57]\n"
     ]
    }
   ],
   "source": [
    "# Character TextSplitter to split documents\n",
    "'''This method splits based on the separator first, then evaluates chunk_size and chunk_overlap to check if it's satisfied.'''\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "ct_splitter = CharacterTextSplitter(\n",
    "    separator = \".\", \n",
    "    chunk_size = chunk_size, \n",
    "    chunk_overlap = chunk_overlap\n",
    ")\n",
    "\n",
    "docs = ct_splitter.split_text(quote)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have a problem: each of these chunks contains more characters than our specified chunk_size. CharacterTextSplitter splits on the separator in an attempt to make chunks smaller than chunk_size, but in this case, splitting on the separator was unable to return chunks below our chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 52, which is longer than the specified 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the work of fifty ordinary humans', 'But no machine can do the work of one extraordinary human']\n",
      "[52, 57]\n"
     ]
    }
   ],
   "source": [
    "# Recursive Character TextSplitter to split documents\n",
    "'''splitting the document using each separator in turn, and seeing if these chunks can be combined while remaining under chunk_size.'''\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n,\", \"\\n\", \" \", \"\"], \n",
    "    chunk_size = chunk_size, \n",
    "    chunk_overlap = chunk_overlap\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_text(quote)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this recursive implementation may work better on larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Character TextSplitter with HTML\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = UnstructuredHTMLLoader(\"white_house_executive_order_nov_2023.html\") # Load the PDF file\n",
    "data = loader.load() # Load the PDF file into memory\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap,\n",
    "    separators = [\".\"]\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_documents(data)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a chroma vector database\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# create an embedding model from openai\n",
    "embedding_function = OpenAIEmbeddings(api_key = 'openai_api_key', model = 'text-embedding-3-small')\n",
    "\n",
    "# create a chroma vector store \n",
    "# to create a Chroma database from a set of documents, call the .from_documents() method on the Chroma class, passing the documents and embedding function to use.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs, \n",
    "    embedding = embedding_function,\n",
    "    persist_directory = \"path/to/directory\"\n",
    ")\n",
    "\n",
    "# integrate the database with other LangChain components\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"similarity\", # perform similarity search\n",
    "    search_kwargs = {\"k\":2} # return the top 2 most similar documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "message = '''\n",
    "Review and fix the following TechStack marketing copy with the following guidelines in consideration:\n",
    "\n",
    "Guidlines:\n",
    "{guidelines}\n",
    "\n",
    "Copy:\n",
    "{copy}\n",
    "\n",
    "Fixed Copy:\n",
    "'''\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"human\", message)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chainging all together\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = ({\"guidelines\": retriever, \"copy\": RunnablePassthrough} \n",
    "             | prompt_template \n",
    "             | llm)\n",
    "\n",
    "rag_chain.invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the documents and vector database\n",
    "loader = PyPDFLoader('rag_vs_fine_tuning.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "# Split the document using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n,\", \"\\n\", \" \", \"\"], \n",
    "    chunk_size = 300, \n",
    "    chunk_overlap =50\n",
    ")\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Embed the documents in a persistent Chroma vector database\n",
    "embedding_function = OpenAIEmbeddings(api_key='<OPENAI_API_TOKEN>', model='text-embedding-3-small')\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=os.getcwd()\n",
    ")\n",
    "\n",
    "# Configure the vector store as a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Prompt\n",
    "# Add placeholders to the message string\n",
    "message = \"\"\"\n",
    "Answer the following question using the context provided:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a chat prompt template from the message string\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"human\", message)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RAG chain\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=OpenAIEmbeddings(api_key='<OPENAI_API_TOKEN>', model='text-embedding-3-small'),\n",
    "    persist_directory=os.getcwd()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# Create a chain to link retriever, prompt_template, and llm\n",
    "rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm)\n",
    "\n",
    "# Invoke the chain\n",
    "response = rag_chain.invoke(\"Which popular LLMs were considered in the paper?\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
