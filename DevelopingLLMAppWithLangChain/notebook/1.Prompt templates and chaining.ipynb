{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template and Chaining - Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyith\\AppData\\Local\\Temp\\ipykernel_44876\\2918683206.py:12: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
      "C:\\Users\\kyith\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\kyith\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LangChain provides a suite of LLM-specific libraries and tools, which can significantly reduce the time and effort required to develop and maintain LLM applications. These libraries cover various aspects of LLM development, including document generation, model compilation, and rule-based programming. By using these libraries, LLM developers can focus on the core tasks of their applications and avoid manual coding, ultimately resulting in faster development times and higher quality applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "\n",
    "# Set your Hugging Face API token\n",
    "huggingfacehub_api_token = \"\"\n",
    "\n",
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt_template = PromptTemplate(template = template, input_variables = [\"question\"])\n",
    "\n",
    "# Create a chain to integrate the prompt template and LLM\n",
    "llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "llm_chain = prompt_template | llm\n",
    "\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "print(llm_chain.invoke({\"question\": question})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Prompt Template - OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define an OpenAI chat model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key='<OPENAI_API_TOKEN>')\t\t\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain the prompt template and model, and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "response = llm_chain.invoke({\"question\": \"How can I retain learning?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the examples list of dicts\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"How many DataCamp courses has Jack completed?\",\n",
    "        \"answer\": \"36\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How much XP does Jack have on DataCamp?\",\n",
    "        \"answer\": \"284,320XP\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What technology does Jack learn about most on DataCamp?\",\n",
    "        \"answer\": \"Python\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many DataCamp courses has Jack completed?\n",
      "36\n",
      "\n",
      "Question: How much XP does Jack have on DataCamp?\n",
      "284,320XP\n",
      "\n",
      "Question: What technology does Jack learn about most on DataCamp?\n",
      "Python\n",
      "\n",
      "Question: What is Jack's favorite technology on DataCamp?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Complete the prompt for formatting answers\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
    "\n",
    "# Create the few-shot prompt\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "# Invoke the prompt template\n",
    "prompt = prompt_template.invoke({\"input\": \"What is Jack's favorite technology on DataCamp?\"})\n",
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "# Create an OpenAI chat LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key='<OPENAI_API_TOKEN>')\n",
    "\n",
    "# Create and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "print(llm_chain.invoke({\"input\": \"What is Jack's favorite technology on DataCamp?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='I want to learn how to play golf. Can you suggest how I can learn this step-by-step?'\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template that takes an input activity\n",
    "learning_prompt = PromptTemplate(\n",
    "    input_variables=[\"activity\"],\n",
    "    template=\"I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?\"\n",
    ")\n",
    "\n",
    "# Create a prompt template that places a time constraint on the output\n",
    "time_prompt = PromptTemplate(\n",
    "    input_variables = [\"learning_plan\"],\n",
    "    template=\"I only have one week. Can you create a plan to help me hit this goal: {learning_plan}.\"\n",
    ")\n",
    "\n",
    "# Invoke the learning_prompt with an activity\n",
    "print(learning_prompt.invoke({\"activity\": \"play golf\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Complete the sequential chain with LCEL\n",
    "seq_chain = ({\"learning_plan\": learning_prompt | llm | StrOutputParser()}\n",
    "    | time_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "# Call the chain\n",
    "print(seq_chain.invoke({\"activity\": \"play golf\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement numexp (from versions: none)\n",
      "ERROR: No matching distribution found for numexp\n"
     ]
    }
   ],
   "source": [
    "# %pip install langgraph == 0.066\n",
    "%pip install numexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key='OpenAI_API_TOKEN') \n",
    "\n",
    "tools = load_tools([\"llm-math\"], llm  = llm)\n",
    "agent = create_react_agent(tools = tools) # Create an agent with the tools\n",
    "\n",
    "messages = agent.invoke({\"messages:\"[(\"human\",\"What is the square root of 101?\")]}) # Invoke the agent\n",
    "print(messages)\n",
    "print(messages[messages][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools\n",
    "tools = load_tools([\"wikipedia\"])\n",
    "\n",
    "# Define the agent\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent.invoke({\"messages\": [(\"human\", \"How many people live in New York City?\")]})\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom tools for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tools for agent\n",
    "\n",
    "# tool formats\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "\n",
    "tools = load_tools([\"llm-math\", \"llm-wikipedia\"], llm = llm)\n",
    "print(tools[0].name) #get the name of the first tool\n",
    "print(tools[0].format) #get the format of the first tool\n",
    "print(tools[0].description) #get the description of the first tool\n",
    "\n",
    "print(tools[0].return_direction) #get the return direction of the first tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Report for Lemonade stand:\n",
      "Revenue: $100\n",
      "Expenses: $50\n",
      "Net Income: $50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# want to define a Python function to generate a financial report for a company. \n",
    "# It takes three arguments: the company_name, revenue, and expenses, \n",
    "# and outputs a string containing the net_income.\n",
    "\n",
    "def financial_report(company_name:str, revenue:int, expenses:int) -> str:\n",
    "    '''Generate a financial report for a company that calculates the net income.'''\n",
    "    net_income = revenue - expenses\n",
    "\n",
    "    report = f\"Financial Report for {company_name}:\\n\"\n",
    "    report += f\"Revenue: ${revenue}\\n\"\n",
    "    report += f\"Expenses: ${expenses}\\n\"\n",
    "    report += f\"Net Income: ${net_income}\\n\"\n",
    "    return report\n",
    "\n",
    "# calling the function\n",
    "# print(financial_report(\"ABC Corp\", 100000, 75000))\n",
    "print(financial_report(company_name=\"Lemonade stand\", revenue=100, expenses=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "financial_report\n",
      "Generate a financial report for a company that calculates the net income.\n",
      "False\n",
      "{'company_name': {'title': 'Company Name', 'type': 'string'}, 'revenue': {'title': 'Revenue', 'type': 'integer'}, 'expenses': {'title': 'Expenses', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "# from functions to tools\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def financial_report(company_name:str, revenue:int, expenses:int) -> str:\n",
    "    '''Generate a financial report for a company that calculates the net income.'''\n",
    "    net_income = revenue - expenses\n",
    "\n",
    "    report = f\"Financial Report for {company_name}:\\n\"\n",
    "    report += f\"Revenue: ${revenue}\\n\"\n",
    "    report += f\"Expenses: ${expenses}\\n\"\n",
    "    report += f\"Net Income: ${net_income}\\n\"\n",
    "    return report\n",
    "\n",
    "print(financial_report.name) #get the name of the tool\n",
    "print(financial_report.description) #get the desctiption of the tool\n",
    "print(financial_report.return_direct) #get the return direction of the tool\n",
    "print(financial_report.args) #get the arguments of the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ReAct agent\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key, temperature=0)\n",
    "agent = create_react_agent(llm, [financial_report])\n",
    "\n",
    "messages = agent.invoke({\"messages\": [(\"human\", \"Generate a financial report for ABC Corp with revenue of $100,000 and expenses of $75,000.\")]})\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
